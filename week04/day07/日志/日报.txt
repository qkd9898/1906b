--------2020-06-01---------
1、redis缓存首页大广告
    2、缓存同步
    3、ElasticSearch介绍
        3.1、索引结构
        3.2、倒排索引
    4、ElasticSearch的安装
        4.1、环境的需求
        4.2、安装ES
        4.3、配置文件
        4.4、启动ES
        4.5、测试
    5、Kibana的介绍
    6、安装Kibana
    7、head的介绍
    8、安装head
    9、ES入门
        9.1、index的管理  创建  删除  修改
        9.2、mapping的管理  创建  查询  更新  删除
        9.3、document的管理  创建 post  put  查询  删除
    10、ES的读写原理
--------2020-06-02---------
 1.ElasticSearch 介绍
    1.1.为什么要用ElasticSearch？
        处理海量数据(全文检索)
    1.2.ElasticSearch介绍
        ElasticSearch是一个基于Lucene的搜索服务器。
    1.3.Lucene->ES的发展
    1.4.原理与应用
        1.4.1.索引结构
        1.4.2.倒排索引
            倒排索引（Inverted index）:也常被称为反向索引，倒排索引是从关键字到文档的映射（已知关键字求文档）。
            逻辑结构部分是一个倒排索引表，由三部分组成：
            1、将搜索的文档最终以Document方式存储起来。
            2、将要搜索的文档内容分词，所有不重复的词组成分词列表。
            3、每个分词和docment都有关联。
        1.4.3.RESTful应用方法
            Elasticsearch提供 RESTful Api接口进行索引、搜索，并且支持多种客户端。
 2.安装 ElasticSearch
    2.1.环境需求
    2.2.安装ES
    2.3.配置文件
    2.4.启动ES
    2.5.测试
 3.安装Kibana
    3.1.什么是Kibana
    3.2.下载
    3.3.安装
    3.4.修改配置
    3.5.启动
    3.6.测试
 4.安装head
    4.1.什么是head
    4.2.安装
    4.3.测试
 5.ES快速入门
    5.1.index管理
        5.1.1.创建index
        5.1.2.修改index
        5.1.3.删除index
    5.2.mapping管理
        5.2.1 创建mapping
        5.2.2.查询mapping
        5.2.3.更新mapping
        5.2.4.删除mapping
    5.3.document管理
        5.3.1.创建document
            5.3.1.1.POST语法
            5.3.1.2.PUT语法
        5.3.2.查询document
        5.3.3.删除Document
    5.4.ES读写原理
        5.4.1.documnet routing（数据路由）
        5.4.2.ES document写操作原理
        5.4.3.ES document读操作原理
        5.4.4.为什么primary shard数量不可变
 6.IK分词器
    6.1.测试分词器
    6.2.中文分词器
        6.2.1.Lucene自带中文分词器
        6.2.2.第三方中文分析器
        6.3.安装IK分词器
        6.4.两种分词模式
        6.5.自定义词库
 7.field详细介绍
    7.1.field的属性介绍
        7.1.1.type： 通过type属性指定field的类型。
        7.1.2.analyzer： 通过analyzer属性指定分词模式。
        7.1.3.index： 通过index属性指定是否索引。
        7.1.4.field索引不存储
    7.2.常用field类型
        7.2.1.text文本字段
        7.2.2.keyword关键字字段
        7.2.3.date日期类型
        7.2.4.Numeric类型
    7.3.总结
        field属性的设置标准：
              	标准
        分词  	是否有意义
        索引  	是否搜索
        存储  	是否展示
8.Spring Boot整合ElasticSearch
    8.1.ES客户端
    8.2.搭建工程
        8.2.1.pom.xml
        8.2.2.application.yml
        8.2.3.config
        8.2.4.app
    8.3.索引管理
        8.3.1.创建索引库
            8.3.1.1.api
            8.3.1.2.Java Client
        8.3.2.删除索引库
            8.3.2.1.api
            8.3.2.2.java client
--------2020-06-03---------
         1、文档的管理
        1.1、添加文档
        1.2、批量添加
        1.3、修改文本
        1.4、删除文本
    2、文档搜索
        2.1、match_all查询  全部查询
        2.2、分页查询
        2.3、match查询  单条件查询
        2.4、multi_match查询   根据一个分词  查询多个字段
        2.5、boot查询   多条件查询
        2.6、filter查询  不需要计算相关度分数
--------2020-06-04---------
es集群搭建
   a、集群结构：es集群至少两台
   b、搭建步骤
     1、拷贝节点2，命名为ElasticSearch-2
     2、删除节点2的data目录
     3、修改elasticsearch.yml：
		node.name: usian_node_2
		discovery.zen.ping.unicast.hosts: ["192.168.205.131:9300", "192.168.205.132:9300"]
     4、测试
	 启动两台节点：自动分配分片
	 关闭节点2：集群是高可用的
	 创建备份分配：备份分配和主分片不在同一个节点上
商品搜索
创建：usian_search_web
     usian_search_feign
     usian_search_service
在usian_search_service的配置文件里配置 ES集群
完成 商品一键导入功能
--------2020-06-05---------
商品的搜索
商品的索引库同步
--------2020-06-08---------
 1、商品的详细信息
        1.1、thymeleaf页面静态化
              创建商品详情的thymeleaf模板
              创建RabbitMQ消费者，收到消息后生成静态页面（D:/detail/26774635180.html）
              搭建nginx服务器，返回静态页面
        1.2、redis
              redis缓存商品详情
              先查询redis，如果有直接返回
              再查询mysql，并把查询结果装到redis中再返回
    2、缓存的同步
        在我们修改商品或者删除商品的时候删除redis中商品的详细信息
    3、缓存的穿透
        就是我们查询一条数据时 查询redis缓存没有 查询数据库也没有
        这个时候我们需要在他查询时一个空值的时候也进行缓存一个空值
    4、缓存的击穿
        当我们redis中的key值失效时，会有大量的用户访问数据库，这样就造成了击穿
        我们要加一个分布式锁解决击穿问题
--------2020-06-09---------
    1、单点登录
      	1.1、什么是单点登录？
      		SSO英文全称Single Sign On，单点登录，用户登陆一次就可以访问全部
      	2、思路
      		存(登录)：redis(token,user)       cookie(token_key,token)
      		取(查询用户)：cookie(token)--------->redis(user)
    2、工程的搭建
    3、注册信息的校验  校验手机号和用户名存在不存在
    4、用户的注册
    5、用户的登录  登录成功把数据存入redis中
    6、通过token查询用户信息  如果redis中有数据  那么展示用户
    7、退出登录  删除redis中的缓存，这样前台也会删除session
--------2020-06-10---------
    1、购物车的分析
        我们的购物车数据是用map集合来存储的，有未登录状态的购物车和登陆后的购物车
        未登录的数据存在cookie中  登录后的数据存在redis中存着
    2、未登录状态的购物车
        添加购物车   查询cookie中商品列表   添加商品到购物车  再把商品列表添加到cookie中
        查看购物车   把cookie中的商品列表遍历给list集合返回前台
        修改购物车   把商品列表中的某一件商品数量进行修改
        、登录后的购物车
        添加购物车   查询redis中商品列表   添加商品到购物车  再把商品列表添加到reids中
        查看购物车   把redis中的商品列表遍历给list集合返回前台
        修改购物车   把商品列表中的某一件商品数量进行修改
        删除购物车   根据商品的id删除商品列表中的数据
---------------2020-06-16----------------
购物车登陆状态下的查询修改删除
订单模块
在购物车完成结算时点下的按钮会跳转到订单确认页面（模块）
会展示商品的列表
订单会确认用户的地址等信息，在展示订单的时候 会确认是否是登陆状态
如果是登陆状态会直接跳转到订单的确认界面
如果不是登陆状态会跳转到登陆页面
---------------2020-06-17----------------
 1、订单的提交
    涉及到三张表  订单的详细表  物流表  订单表
 2、库存的同步
     当我们提交订单之后把数据库的数据修改、
---------------2020-06-18----------------
    1、订单过期
        当我们有的订单没有支付，并且超过2天，我们就把订单关闭
        我们就用到了quartz定时任务，每五秒查看一次，把过期的订单关闭
    2、quartz集群的重复执行问题
        这时候我们就用到setnx分布式锁 解决quartz集群重复执行问题
        用setnx来判断是否有人使用锁 并且给一个自动销毁的时间（30）来防止死锁
---------------2020-06-19----------------
    1、事务的介绍
        同时执行多条sql语句，要么同时完成，要么同时失败
    2、本地事务
        数据库控制事务   jdbc控制事务   aop控制事务
    3、分布式事务
        把一个应用分成可独立部署的多个服务，因此需要服务与服务之间远程协作才能完成实务操作
    4、分布式事务产生的场景
        两个service---一个数据库
        一个service---多个数据库
    5、RabbitMQ可靠消息最终一致性介绍
        可靠消息：消息成功消费
        最终一致性：事务参与方最终完成事务
    6、可靠消息最终一致性要解决的问题
        上游服务消息发送成功
        下游服务消息消费成功
        对消息幂等
---------------2020-06-20----------------
    1、分布式日志
        ELK是Elasticsearch,logstash, kibana
        Elasticsearch:是开源的分布式全文检索服务器
        logstash：解析日志，并发送数据到ES
        kibana:数据分析
    2、安装Logstash
        修改配置文件vim config/log_to_es.conf   启动
    3、springcloud 集成elk
        在pom.xml引入  logstash-logback-encoder
        在logback.xml进行修改  指定logstash的端口和ip
    4、测试
        通过kibana分析日志
---------------2020-06-24----------------
    1、主从复制
	    安装mysql
		a、解压
			tar -zxvf mysql-5.6.31-linux-glibc2.5-x86_64.tar.gz -C /usr/java
			mv mysql-5.6.31-linux-glibc2.5-x86_64 mysql
                b、拷贝配置文件到etc
			cp support-files/my-default.cnf /etc/my.cnf
			cp support-files/mysql.server /etc/rc.d/init.d/mysql
			vim /etc/my.cnf
				basedir = /usr/java/mysql
				datadir = /usr/java/mysql/data
				log-error = /usr/java/mysql/data/error.log
				pid-file = /usr/java/mysql/data/mysql.pid
				user = root
				tmpdir = /tmp
		c、初始化mysql
			./scripts/mysql_install_db --user=root --basedir=/usr/java/mysql --datadir=/usr/java/mysql/data --pid-file=/usr/java/mysql/data/mysql.pid --tmpdir=/tmp

		d、配置mysql命令支持
			ln -s /usr/java/mysql/bin/mysql /usr/bin/mysql
		e、修改密码
			mysql -u root
			use mysql;
			update user set password= password("1111") where user='root';
			flush privileges;
		f、开启远程登录权限
			GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '1111' WITH GRANT OPTION;
			flush privileges;
		g、设置开机启动
			chkconfig mysql on
		d、启动和关闭mysql
			service mysql start|stop|restart

	2、配置mysql主从复制
		a、配置主数据库
			开启日志、修改server_id
				vim /etc/my.cnf：
					log_bin=master_log
					server_id=1

		b、配置从数据库
			修改server_id：
				vim /etc/my.cnf：
					server_id=1
			修改uuid：
				vim mysql/data/auto.cnf：
					uuid=xxxxxxxxx
			重启：
				service mysql restart
		c、修改slave
				mysql> change master to master_host='192.168.204.139',master_user='root',master_password='1111',master_log_file='master_log.00001'

		d、查看主从状态
			mysql> show slave status \G;
    3、什么是MyCat？
    		是一个国产的数据库中间件，前身是阿里的cobar。
    	3.1、分库分表
    		分库：把usian拆成多个库
    		分表：把tb_order拆分到多个库里

    	3.2、mycat的核心概念
    		schema：逻辑上完整的库
    		table：逻辑上完整的表
    		dataHost：服务器
    		dataNode：服务器上的mysql
    		rule：分片规则
    	3.3、分片规则
    		crc32slot规则：
    			分片字段使用主键
    			tableRule：一个表一个
    			数据库节点数量
    	3.4、配置mycat的分库分表和读写分离
    		a、schema.xml作用：逻辑库、逻辑表、dataNode、分片规则
    		b、rule.xml：分片规则
    		c、server.xml：mycat的用户名、密码和权限
---------------2020-06-28----------------
    1、swagger
        是一个实现了OpenAPI规范的工具集，用于生成API文档并提供可视化 RESTful 风格的 Web 服务。
        OpenAPI规范（OpenAPI Specification 简称OAS）是Linux基金会的一个项目，试图通过定义一种用来描述API格式或API定义的语言，
        来规范RESTful服务开发过程。目前V3.0版本的OpenAPI规范已经发布并开源在github上
      为什么使用swagger？
        随着互联网技术的发展，现在的网站架构基本都由原来的后端渲染，变成了：前端渲染、前后端分离的形态，而且前端技术和后端技术在各自的道路上越走越远。 
        前端和后端的唯一联系，变成了API接口；API文档变成了前后端开发人员联系的纽带，变得越来越重要。
        没有API文档工具之前，大家都是手写API文档的，在什么地方书写的都有，而且API文档没有统一规范和格式，每个公司都不一样
      常用的注解
         @Api：修饰整个类，描述Controller的作用
         @ApiOperation：描述一个类的一个方法，或者说一个接口
         @ApiParam：单个参数描述
         @ApiImplicitParam：单个参数描述
         @ApiImplicitParams：多个参数描述
         @ApiModel：用对象来接收参数
         @ApiModelProperty：用对象接收参数时，描述对象的一个字段
         @ApiResponse：HTTP响应其中1个描述
         @ApiResponses：HTTP响应整体描述
    2、网关服务
        需求    路由：所有的请求都通过网关服务的consumer
                容错：客户端通过zuul无法调用consumer时，使用zuul对consumer进行降级
                限流：使用令牌桶算法实现zuul对consumer的限流